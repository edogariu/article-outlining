{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS AND ALL THAT"
      ],
      "metadata": {
        "id": "ZvlUS3gGFu9f"
      },
      "id": "ZvlUS3gGFu9f"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "\n",
        "!pip install simcse\n",
        "\n",
        "!pip install gensim==4.1.2\n",
        "!pip install cython\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "aucLqNev5jNX"
      },
      "id": "aucLqNev5jNX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import random\n",
        "random.seed(10)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "from IPython.utils import io\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from simcse import SimCSE\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o-FM39kEF4yu"
      },
      "id": "o-FM39kEF4yu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to add 484-finalProject as shortcut in drive."
      ],
      "metadata": {
        "id": "xiGy1ifZGDOw"
      },
      "id": "xiGy1ifZGDOw"
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/My\\ Drive/484-finalProject"
      ],
      "metadata": {
        "id": "KA9TfA1RGB6n"
      },
      "id": "KA9TfA1RGB6n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PREPARE DATASET"
      ],
      "metadata": {
        "id": "2PJZlXY0CR7F"
      },
      "id": "2PJZlXY0CR7F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb33fc8",
      "metadata": {
        "id": "bfb33fc8"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, list_datasets\n",
        "import pandas as pd \n",
        "import re \n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb659e0",
      "metadata": {
        "id": "3bb659e0"
      },
      "outputs": [],
      "source": [
        "class Node(object):\n",
        "    '''\n",
        "    each node contains \n",
        "    - parent \n",
        "    - children \n",
        "    - text\n",
        "    '''\n",
        "    def __init__(self,txt: str, level:int):\n",
        "        self.text = txt \n",
        "        self.level = level\n",
        "        self.parent = None \n",
        "        self.children = []\n",
        "    def insertChild(self,child):\n",
        "        self.children.append(child)\n",
        "    def linkParent(self, parent):\n",
        "        if(self.parent != None):\n",
        "            print(\"ERROR: node \", self.text, \"already has a parent\")\n",
        "        else:\n",
        "            self.parent = parent \n",
        "            \n",
        "        \n",
        "class Tree(object):\n",
        "    def __init__(self,document):\n",
        "        self.root = Node(document['title'],level=0)\n",
        "        self.depth = np.amax([v['type'] for v in document['document']], initial=0)\n",
        "        curNode = self.root \n",
        "        # para of format {\"text\", \"type\"}\n",
        "        for para in document['document']:\n",
        "            newNode = Node(para['text'],para['type'])\n",
        "            \n",
        "            # growing in depth\n",
        "            if(newNode.level == -1 or newNode.level > curNode.level):\n",
        "                curNode.insertChild(newNode)\n",
        "                newNode.linkParent(curNode)\n",
        "                if(newNode.level > 0):\n",
        "                    curNode = newNode \n",
        "                \n",
        "            # new heading belong to the same or lower level of subheading \n",
        "            else: \n",
        "                # trace back to the heading level that new heading is immediately under \n",
        "                while(curNode.level>=newNode.level):\n",
        "                    curNode = curNode.parent\n",
        "                curNode.insertChild(newNode)\n",
        "                newNode.linkParent(curNode)\n",
        "                curNode = newNode \n",
        "        return \n",
        "    \n",
        "    def printTree(self):\n",
        "        print(\"======== PRINTING TREE =========\")\n",
        "        print(\"TITLE: \", self.root.text)\n",
        "        print(\"MAX DEPTH: \", self.depth)\n",
        "        print(\"===============================\")\n",
        "        def printNode(curNode):\n",
        "            print(curNode.text)\n",
        "            if(curNode.level == -1):\n",
        "                return \n",
        "            \n",
        "            for child in curNode.children:\n",
        "                printNode(child)\n",
        "            return \n",
        "        printNode(self.root)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09a8de0",
      "metadata": {
        "id": "f09a8de0"
      },
      "outputs": [],
      "source": [
        "## helper functions \n",
        "\n",
        "# get type of text \n",
        "def checkHeading(txt):\n",
        "    if(txt == ''):\n",
        "        return -2\n",
        "    if(re.search(r'^\\s=.+\\s=\\s\\n',txt)):\n",
        "        return int(len(re.findall(r'\\s=',txt))/2 - 1)\n",
        "    return -1 \n",
        "\n",
        "# load documents to feed to tree \n",
        "def createDocuments(data):\n",
        "    documents_with = []\n",
        "    documents_without = []\n",
        "    document_with = []\n",
        "    document_without = []\n",
        "    curTitle = ''\n",
        "    for i in data:\n",
        "        c = checkHeading(i)\n",
        "        if(c==-2):\n",
        "            continue\n",
        "        if(c>-1):\n",
        "            # strip heading \n",
        "            i = re.findall(r'=\\s([^=]+)\\s=', i)[0]\n",
        "        if(c==0):\n",
        "            \n",
        "            # clear out empty headings \n",
        "            while(len(document_with)>1 and document_with[-1]['type']!=-1):\n",
        "                document_with.pop(-1)\n",
        "            documents_with.append({'title': curTitle, 'document':document_with})\n",
        "            documents_without.append(document_without)\n",
        "            curTitle = i\n",
        "            document_with = []\n",
        "            document_without = []\n",
        "            \n",
        "        else:\n",
        "            # clear out empty headings GOOFY HELP HOW TO CLEAN THIS UP \n",
        "            if(len(document_with)>1 and document_with[-1]['type']!=-1 and c <= document_with[-1]['type'] and c!=-1):\n",
        "                document_with.pop(-1)\n",
        "            document_with.append({'text':i,'type':c})\n",
        "            if(c==-1):\n",
        "                document_without.append(i)\n",
        "            \n",
        "    documents_with.pop(0)\n",
        "    documents_without.pop(0)\n",
        "    return documents_with, documents_without"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ac6222",
      "metadata": {
        "id": "e7ac6222"
      },
      "source": [
        "loadData() creates a list of data points containing the title of article, raw text (paragraphs), and the tree representation of heading structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f4ecfe",
      "metadata": {
        "id": "e6f4ecfe"
      },
      "outputs": [],
      "source": [
        "## load wiki dataset \n",
        "def loadData(train = False, min_size=-1):\n",
        "    \"\"\"\n",
        "    prepare dataset for training, which is a list of dictionaries containing:  \n",
        "    - document title (string)\n",
        "    - paragraphs (list of string)\n",
        "    - tree representation of headings\n",
        "    \"\"\"\n",
        "    \n",
        "    data_raw = load_dataset(\"wikitext\",'wikitext-103-v1',split='train' if train else 'test')\n",
        "    data_raw = data_raw['text']\n",
        "    documents_with, documents_without = createDocuments(data_raw)\n",
        "    \n",
        "    data = []\n",
        "    i = 0\n",
        "    for document in documents_with:\n",
        "        tree = Tree(document)\n",
        "        if len(documents_without[i]) < min_size:\n",
        "          continue\n",
        "#         tree.printTree()\n",
        "        data.append({\n",
        "            \"title\":document['title'],\n",
        "            \"paragraphs\":documents_without[i],\n",
        "            \"tree\": tree\n",
        "        })\n",
        "        i+=1\n",
        "    return data    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5fbda0",
      "metadata": {
        "scrolled": true,
        "id": "de5fbda0"
      },
      "outputs": [],
      "source": [
        "data = loadData(train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA LCA"
      ],
      "metadata": {
        "id": "7dU8iPy-CgQO"
      },
      "id": "7dU8iPy-CgQO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Preparing LCA loss evaluation function + tools for trees (please run!)"
      ],
      "metadata": {
        "id": "PDWF2apsjZT1"
      },
      "id": "PDWF2apsjZT1"
    },
    {
      "cell_type": "code",
      "source": [
        "# tree-related helper functions\n",
        "\n",
        "# iterate over a tree rooted at node in preorder traversal\n",
        "def preorder(node):\n",
        "    if len(node.children) == 0:\n",
        "        yield node\n",
        "    for ch in node.children:\n",
        "        yield from preorder(ch)\n",
        "\n",
        "# only prints leaves, i.e. text representations of paragraph\n",
        "# note: depends on accurate text, level population\n",
        "# text should be indices\n",
        "def print_tree(curNode):\n",
        "    if curNode.level == -1:\n",
        "        print(curNode.text, end='')\n",
        "        return\n",
        "    print('[', end='')\n",
        "    for idx, child in enumerate(curNode.children):\n",
        "        print_tree(child)\n",
        "        if idx < len(curNode.children) - 1:\n",
        "            print(', ', end='')\n",
        "    print(']', end='')\n",
        "    if curNode.level == 0:\n",
        "        print() # final print after entire tree is printed\n",
        "\n",
        "# text should be snippets\n",
        "def print_snippet_tree(curNode, indent='  '):\n",
        "    if curNode.level == -1:\n",
        "        print(indent + curNode.text, end='')\n",
        "        return\n",
        "    print(indent+'[heading]')\n",
        "    for idx, child in enumerate(curNode.children):\n",
        "        print_snippet_tree(child, indent+'  ')\n",
        "        if idx < len(curNode.children) - 1:\n",
        "            print()\n",
        "    if curNode.level == 0:\n",
        "        print() # final print after entire tree is printed\n",
        "\n",
        "def clone_tree(root):\n",
        "    root_copy = Node(root.text, root.level)\n",
        "    for ch in root.children:\n",
        "        root_copy.insertChild(clone_tree(ch))\n",
        "        root_copy.children[-1].linkParent(root_copy)\n",
        "    return root_copy\n",
        "\n",
        "# return indexified tree with text as paragraphs to text as indices of paragraphs for more concise printing\n",
        "def indexified_tree(root):\n",
        "    root_copy = clone_tree(root)\n",
        "    for idx, node in enumerate(preorder(root_copy)):\n",
        "        node.text = idx\n",
        "    return root_copy\n",
        "\n",
        "# return indexified tree with text as paragraphs to text as indices of paragraphs for more concise printing\n",
        "def textified_tree(root, paras):\n",
        "    root_copy = clone_tree(root)\n",
        "    for idx, node in enumerate(preorder(root_copy)):\n",
        "        node.text = paras[idx][:40] + '...'\n",
        "    return root_copy\n",
        "\n",
        "# print_tree(roots[0])\n",
        "# print_tree(train_y[0].root)"
      ],
      "metadata": {
        "id": "IBXXXBS4jjcT"
      },
      "id": "IBXXXBS4jjcT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lca-related helper functions and lca loss\n",
        "# note: assumed indexified trees\n",
        "def trace_helper(node, i, trace):\n",
        "    if node.text == i:\n",
        "        return True\n",
        "    for idx, ch in enumerate(node.children):\n",
        "        if trace_helper(ch, i, trace):\n",
        "            trace.append(idx)\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_trace(root, i):\n",
        "    trace = []\n",
        "    trace_helper(root, i, trace)\n",
        "    trace.reverse()\n",
        "    return trace\n",
        "\n",
        "def compute_lca_dist(root, i, j):\n",
        "    trace_i = get_trace(root, i)\n",
        "    trace_j = get_trace(root, j)\n",
        "    # print(trace_i)\n",
        "    # print(trace_j)\n",
        "    for idx in range(min(len(trace_i), len(trace_j))):\n",
        "        if trace_i[idx] != trace_j[idx]:\n",
        "            return len(trace_i) + len(trace_j) - 2 * idx\n",
        "    return len(trace_i) + len(trace_j)\n",
        "\n",
        "def lca_loss(root1, root2, num_paras):\n",
        "    loss = 0\n",
        "    for i in range(1, num_paras+1): # 1-indexed from indexify\n",
        "        for j in range(i+1, num_paras+1):\n",
        "            dist1 = compute_lca_dist(root1, i, j)\n",
        "            dist2 = compute_lca_dist(root2, i, j)\n",
        "            # print(i, j, dist1, dist2)\n",
        "            loss += (dist1 - dist2) * (dist1 - dist2)\n",
        "    if num_paras == 1:\n",
        "        return loss\n",
        "    return loss / num_paras / (num_paras - 1) * 2\n",
        "\n",
        "def batch_lca_loss(roots1, roots2, num_paras):\n",
        "    tt = 0\n",
        "    for root1, root2, num in zip(roots1, roots2, num_paras):\n",
        "        tt += lca_loss(root1, root2, num)\n",
        "    return tt / len(roots1)"
      ],
      "metadata": {
        "id": "z9hZqeHXjef7"
      },
      "id": "z9hZqeHXjef7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA MODEL / TRAINING"
      ],
      "metadata": {
        "id": "9PbhxTS2XoyW"
      },
      "id": "9PbhxTS2XoyW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model 3: Recursive split MLP -- note this code may have a decoding bug"
      ],
      "metadata": {
        "id": "yc5HGcLzgoFv"
      },
      "id": "yc5HGcLzgoFv"
    },
    {
      "cell_type": "code",
      "source": [
        "def add_tree_breaks(node, breaks, cur_breaks, depths, depth):\n",
        "    if node.level == -1: # this is paragraph node, not a heading\n",
        "        depths.append(depth)\n",
        "        breaks.append(cur_breaks)\n",
        "        return\n",
        "    for child in node.children:\n",
        "        if child == node.children[0]: # first entry\n",
        "            cur_breaks += 1\n",
        "            depth = node.level\n",
        "        if child == node.children[-1]:\n",
        "            cur_breaks = 0\n",
        "            depth = 0\n",
        "        add_tree_breaks(child, breaks, cur_breaks, depths, depth)\n",
        "        if child == node.children[0]:\n",
        "            cur_breaks = 0\n",
        "            depth = 0\n",
        "        if child == node.children[-1]:\n",
        "            cur_breaks = 0\n",
        "            depth = 0\n",
        "    cur_breaks = 0\n",
        "\n",
        "def get_breaks(paras, trees):\n",
        "    max_paras = max([len(x) for x in paras])\n",
        "    breaks = []\n",
        "    depths = []\n",
        "    for tree in trees:\n",
        "        breaks_i = []\n",
        "        depths_i = []\n",
        "        add_tree_breaks(tree.root, breaks_i, 0, depths_i, 0)\n",
        "        breaks_i = torch.tensor(breaks_i)\n",
        "        depths_i[0] += 1\n",
        "        depths_i = torch.tensor(depths_i)\n",
        "        breaks.append(breaks_i)\n",
        "        depths.append(depths_i)\n",
        "    return breaks, depths\n",
        "\n",
        "def convert_dataset(dataset, window_size):\n",
        "  breaks, depths = get_breaks([d['paragraphs'] for d in dataset], [d['tree'] for d in dataset])\n",
        "  X = []\n",
        "  D = []\n",
        "  y = []\n",
        "  for i in tqdm(range(len(dataset))):  # for article\n",
        "      # print('{}'.format(100 * i / len(dataset)))\n",
        "      article = dataset[i]['paragraphs']\n",
        "      for p in range(1, len(article)):  # for para in article (excluding first, since its clearly always a break)\n",
        "          depth = depths[i][p]\n",
        "          isBreak = breaks[i][p]\n",
        "          for b in range(1, depth + 1):  # for depths <= depth of para\n",
        "              context = []\n",
        "              for j in range(p - window_size, p + window_size + 1):  # for para in context\n",
        "                  if j < 0 or j >= len(article):\n",
        "                      context.append(None)\n",
        "                  else:\n",
        "                      context.append(article[j])\n",
        "              X.append(context)\n",
        "              D.append(np.array([b]))\n",
        "              val = 1 if b > depth - isBreak else 0  # 1 if target para is first after break at depth b\n",
        "              y.append(np.array([val]))\n",
        "          if depth == 0:\n",
        "              for b in range(1, 8):  # MAX_DEPTH is 8\n",
        "                  context = []\n",
        "                  for j in range(p - window_size, p + window_size + 1):  # for para in context\n",
        "                      if j < 0 or j >= len(article):\n",
        "                        context.append(None)\n",
        "                      else:\n",
        "                        context.append(article[j])\n",
        "                  X.append(context)\n",
        "                  D.append(np.array([0]))\n",
        "                  y.append(np.array([0]))\n",
        "  return X, torch.tensor(np.stack(D, axis=0)), torch.tensor(np.stack(y, axis=0))"
      ],
      "metadata": {
        "id": "cvpuvYJnhci0"
      },
      "id": "cvpuvYJnhci0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split_data(data, window_size, train_ratio=0.8, val_ratio=0.0001, test_ratio=0.0001):\n",
        "    random.shuffle(data)\n",
        "    n_data = len(data)\n",
        "    train_idx = int(train_ratio * n_data)\n",
        "    val_idx = int((train_ratio + val_ratio) * n_data)\n",
        "    test_idx = int((train_ratio + val_ratio + test_ratio) * n_data)\n",
        "    print('getting training data')\n",
        "    train_data = convert_dataset(data[:train_idx], window_size=window_size)\n",
        "    print('getting val data')\n",
        "    val_data = convert_dataset(data[train_idx:val_idx], window_size=window_size)\n",
        "    print('getting test data')\n",
        "    test_data = convert_dataset(data[val_idx:test_idx], window_size=window_size)\n",
        "    return train_data, val_data, test_data"
      ],
      "metadata": {
        "id": "1Ce1cQI_hjKm"
      },
      "id": "1Ce1cQI_hjKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over batches of data and labels\n",
        "def batch_iter(data, batch_size, shuffle=False):\n",
        "    X, D, y = data\n",
        "    batch_num = math.ceil(len(X) / batch_size)\n",
        "    index_array = list(range(len(X)))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_array)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "        batch_data_X = [X[idx] for idx in indices]\n",
        "        batch_data_D = D[indices]\n",
        "        batch_data_y = y[indices]\n",
        "\n",
        "        yield batch_data_X, batch_data_D, batch_data_y"
      ],
      "metadata": {
        "id": "D5oIYe9shkio"
      },
      "id": "D5oIYe9shkio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODULES FOR EMBEDDING 3 DIFFERENT WAYS: DOC2VEC, PROJECTED SIMCSE, PROJECTED FASTTEXT\n",
        "# each one takes a batch of lists of paragraphs, outputs a batch of concatenated paragraph embeddings\n",
        "# forward pass input: batch (len B) of list of paragraphs (len 2 * window_size + 1), each para variable length\n",
        "# forward pass output: tensor of size B x ((2 * window_size + 1) * emb_dim)\n",
        "class Doc2VecEmbedding(nn.Module):\n",
        "  def __init__(self, window_size, emb_dim): \n",
        "    super().__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.doc2vec = Doc2Vec.load(\"models/{}_d2v.model\".format(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "      batch = []\n",
        "      for b in x:\n",
        "        paras = []\n",
        "        for p in b:\n",
        "          paras.append(self.doc2vec.infer_vector(word_tokenize(p.lower())) if p is not None else np.zeros(shape=(self.emb_dim)))\n",
        "        batch.append(np.concatenate(paras, axis=0))\n",
        "      return torch.tensor(np.stack(batch, axis=0)).to(device)\n",
        "\n",
        "class SimCSEEmbedding(nn.Module):\n",
        "  def __init__(self, window_size, emb_dim, dropout): \n",
        "    super().__init__()\n",
        "    self.window_size = window_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.simcse = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
        "    self.SIMCSE_DIM = 768 # dim of simcse sentence embeddings\n",
        "    self.lstm = nn.LSTM(input_size=self.SIMCSE_DIM,\n",
        "                        hidden_size=int(emb_dim / 4),\n",
        "                        num_layers=1,\n",
        "                        bidirectional=True,\n",
        "                        batch_first=True, \n",
        "                        dropout=0.0).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = len(x)  # batch size\n",
        "    batch = []\n",
        "    with torch.no_grad():\n",
        "      for b in x:\n",
        "        for p in b:\n",
        "          if p is not None:\n",
        "            p = re.sub('\\n', '', p)\n",
        "            sents = re.split('[.]|[!]|[?]', p.strip())\n",
        "            with io.capture_output() as captured:\n",
        "                sents_emb = self.simcse.encode(sents, device=device, batch_size=len(sents), max_length=128)  # a tensor of len(sents) x SIMCSE_DIM\n",
        "            batch.append(sents_emb)\n",
        "          else:\n",
        "            batch.append(torch.zeros((1, self.SIMCSE_DIM), device=device))\n",
        "      batch_emb = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True).to(device)\n",
        "      packed_in = torch.nn.utils.rnn.pack_padded_sequence(batch_emb, torch.tensor([a.shape[0] for a in batch_emb]), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    _, (hidden, cell) = self.lstm(packed_in.to(device))\n",
        "    para_embs = torch.cat((hidden[0], cell[0], hidden[1], cell[1]), dim=1)        \n",
        "      \n",
        "    return para_embs.reshape(B, (2 * self.window_size + 1) * self.emb_dim)\n",
        "\n",
        "\n",
        "#### HERE\n",
        "class FastTextEmbedding(nn.Module):\n",
        "  def __init__(self, window_size, emb_dim, dropout): \n",
        "    super().__init__()\n",
        "    self.window_size = window_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.fasttext = FastText.load_fasttext_format('models/fast-text-300.bin')\n",
        "    self.FASTTEXT_DIM = 300 # dim of fasttext word embeddings\n",
        "    self.lstm = nn.LSTM(input_size=self.FASTTEXT_DIM,\n",
        "                        hidden_size=int(emb_dim / 4),\n",
        "                        num_layers=1,\n",
        "                        bidirectional=True,\n",
        "                        batch_first=True, \n",
        "                        dropout=0.0).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = len(x)  # batch size\n",
        "    batch = []\n",
        "    with torch.no_grad():\n",
        "      for b in x:\n",
        "        for p in b:\n",
        "          if p is not None:\n",
        "            p = re.sub('\\n', '', p)\n",
        "            words = re.sub(\"[^\\s\\w]\", \"\", p.strip()).split(' ')\n",
        "            words = list(filter(None, words))\n",
        "            words_emb = torch.stack([torch.tensor(self.fasttext[word]) for word in words]).to(device)  # a tensor of len(words) x FASTTEXT_DIM\n",
        "            batch.append(words_emb)\n",
        "          else:\n",
        "            batch.append(torch.zeros((1, self.FASTTEXT_DIM), device=device))\n",
        "      batch_emb = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True).to(device)\n",
        "      packed_in = torch.nn.utils.rnn.pack_padded_sequence(batch_emb, torch.tensor([a.shape[0] for a in batch_emb]), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    _, (hidden, cell) = self.lstm(packed_in.to(device))\n",
        "    para_embs = torch.cat((hidden[0], cell[0], hidden[1], cell[1]), dim=1)        \n",
        "      \n",
        "    return para_embs.reshape(B, (2 * self.window_size + 1) * self.emb_dim)"
      ],
      "metadata": {
        "id": "BAtSWSpo5hMJ"
      },
      "id": "BAtSWSpo5hMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, layer_dims, window_size, emb_dim, emb_method, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.emb_dim = emb_dim # dimension of each paragraph embedding\n",
        "        if emb_method == 'doc2vec':\n",
        "          self.emb = Doc2VecEmbedding(window_size, emb_dim)\n",
        "        elif emb_method == 'simcse':\n",
        "          self.emb = SimCSEEmbedding(window_size, emb_dim, dropout=dropout)\n",
        "        elif emb_method == 'fasttext':\n",
        "          self.emb = FastTextEmbedding(window_size, emb_dim, dropout=dropout)\n",
        "        else:\n",
        "          raise NotImplementedError()\n",
        "\n",
        "        in_feats = (2 * window_size + 1) * self.emb_dim\n",
        "        self.layers = []\n",
        "        for dim in layer_dims:\n",
        "          self.layers.append(nn.Linear(in_features=in_feats + 1, out_features=dim))\n",
        "          in_feats = dim\n",
        "        self.layers.append(nn.Linear(in_features=in_feats + 1, out_features=1))\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, d):\n",
        "      '''\n",
        "      x is a batch (list) of windows (list) of paragraphs, which are strings\n",
        "      d is the depths for the whole batch (tensor)\n",
        "      '''\n",
        "      B = len(x)\n",
        "      x = self.emb(x).float()  # x is a (B x (2W + 1) x E) tensor\n",
        "      x = x.reshape(B, (2 * self.window_size + 1) * self.emb_dim)\n",
        "      for layer in self.layers[:-1]:\n",
        "        x = F.relu(layer(torch.cat((x, d), dim=1)))\n",
        "      x = self.dropout(x)\n",
        "      x = torch.sigmoid(self.layers[-1](torch.cat((x, d), dim=1)))\n",
        "      return x\n",
        "\n",
        "  def recursive_outline(self, subarticle, node):\n",
        "    if len(subarticle) == 1:\n",
        "        new = Node(subarticle[0], -1)\n",
        "        new.linkParent(node)\n",
        "        node.insertChild(new)\n",
        "        return\n",
        "    outs = [False]\n",
        "    for p in range(1, len(subarticle)):\n",
        "        context = []\n",
        "        for j in range(p - self.window_size, p + self.window_size + 1):\n",
        "            if j < 0 or j >= len(subarticle):\n",
        "                context.append(None)\n",
        "            else:\n",
        "                context.append(subarticle[j])\n",
        "        X = [context]\n",
        "        D = torch.tensor([node.level + 1]).to(device).unsqueeze(dim=0)\n",
        "        out = self.forward(X, D).squeeze()\n",
        "        outs.append(out.cpu().item() > 0.77)\n",
        "    prev = 0\n",
        "    flag = True\n",
        "    for o in range(len(outs)):\n",
        "        if outs[o]:\n",
        "            new = Node('', node.level + 1)\n",
        "            new.linkParent(node)\n",
        "            node.insertChild(new)\n",
        "            self.recursive_outline(subarticle[prev:o], new)\n",
        "            prev = o\n",
        "            flag = False\n",
        "    if flag:\n",
        "      for p in range(len(subarticle)):\n",
        "        new = Node(subarticle[p], -1)\n",
        "        new.linkParent(node)\n",
        "        node.insertChild(new)\n",
        "\n",
        "        # else:\n",
        "        #     new = Node(subarticle[o], -1)\n",
        "        #     new.linkParent(node)\n",
        "        #     node.insertChild(new)\n",
        "    else:\n",
        "       new = Node('', node.level + 1)\n",
        "       new.linkParent(node)\n",
        "       node.insertChild(new)\n",
        "       self.recursive_outline(subarticle[prev:], new)\n",
        "    return\n",
        "\n",
        "  def outline(self, article, wordy=False):\n",
        "      self.eval()\n",
        "      root = Node('root', 1)\n",
        "      self.recursive_outline(article, root)\n",
        "      new = Node(article[0], -1)\n",
        "      curr = root\n",
        "      while len(curr.children) > 0 and curr.level != -1:\n",
        "        curr = curr.children[0]\n",
        "      new.linkParent(curr.parent)\n",
        "      curr.parent.insertChild(new)\n",
        "      if len(article) > 1:\n",
        "          new = Node(article[len(article)-1], -1)\n",
        "          curr = root\n",
        "          while len(curr.children) > 0 and curr.level != -1:\n",
        "            curr = curr.children[-1]\n",
        "          new.linkParent(curr.parent)\n",
        "          curr.parent.insertChild(new)\n",
        "\n",
        "      def printNode(curNode):\n",
        "          print(curNode.level, '       ', curNode.text)\n",
        "          if curNode.level == -1:\n",
        "              return\n",
        "\n",
        "          for child in curNode.children:\n",
        "              printNode(child)\n",
        "          return\n",
        "\n",
        "      if wordy:\n",
        "        printNode(root)\n",
        "      return root\n",
        "\n",
        "  def save(self, path: str):\n",
        "      \"\"\" Save the model to a file.\n",
        "      @param path (str): path to the model\n",
        "      \"\"\"\n",
        "\n",
        "      params = {\n",
        "          # 'args': dict(hid_dim=self.hid_dim, n_layers=self.n_layers, num_heads=self.num_heads,\n",
        "          #       num_enc_layers=self.num_enc_layers, num_dec_layers=self.num_dec_layers, ff_dim=self.ff_dim, dropout=self.dropout),\n",
        "          'state_dict': self.state_dict()\n",
        "      }\n",
        "\n",
        "      torch.save(params, path)"
      ],
      "metadata": {
        "id": "K2ayqAuIhqjt"
      },
      "id": "K2ayqAuIhqjt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, val_data, lr=0.002, batch_size=32, grad_clip=5.0, lr_decay=0.5,\n",
        "          max_epoch=50, log_every=5, valid_niter=25, max_patience=4, max_num_trial=5, model_path='mlp.bin'):\n",
        "    model.train()\n",
        "    model.float()\n",
        "\n",
        "    # # initialize model parameters\n",
        "    # for p in model.parameters():\n",
        "    #     p.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('use device: %s' % device)\n",
        "\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    print('{} parameters!'.format(sum([np.prod(p.size()) for p in model_parameters])))\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "    num_trial = 0\n",
        "    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
        "    cum_examples = report_examples = epoch = valid_num = 0\n",
        "    hist_valid_scores = []\n",
        "    train_time = begin_time = time.time()\n",
        "    print('begin Maximum Likelihood training')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    loss_fn = nn.BCELoss(reduction='sum')\n",
        "    \n",
        "    while True:\n",
        "        epoch += 1\n",
        "        batch_num = math.ceil(len(train_data[0]) / batch_size)\n",
        "        current_iter = 0\n",
        "        for batch in batch_iter(train_data, batch_size=batch_size, shuffle=True):\n",
        "            X, D, y = batch\n",
        "            D = D.to(device)\n",
        "            y = y.to(dtype=torch.float32, device=device)\n",
        "\n",
        "            model.train()\n",
        "            current_iter += 1\n",
        "            train_iter += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch_size = len(X)\n",
        "            out = model(X, D)\n",
        "            train_loss = loss_fn(out, y)\n",
        "            train_loss.backward()\n",
        "\n",
        "            # clip gradient\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            report_loss += train_loss.item()\n",
        "            cum_loss += train_loss.item()\n",
        "            report_examples += batch_size\n",
        "            cum_examples += batch_size\n",
        "\n",
        "            if train_iter % log_every == 0:\n",
        "                print('epoch %d (%d / %d), iter %d, avg train loss %f, '\n",
        "                      'cum examples %d, time elapsed %.2f sec' %\n",
        "                      (epoch, current_iter, batch_num, train_iter,\n",
        "                       report_loss / report_examples,\n",
        "                       cum_examples,\n",
        "                       time.time() - begin_time))\n",
        "\n",
        "                train_time = time.time()\n",
        "                report_loss = report_examples = 0.\n",
        "\n",
        "            # perform validation\n",
        "            if train_iter % valid_niter == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    print('epoch %d, iter %d, cum loss %f, cum examples %d' % (epoch, train_iter,\n",
        "                            cum_loss / cum_examples,\n",
        "                            cum_examples))\n",
        "                    train_losses.append(cum_loss / cum_examples)\n",
        "                    cum_loss = cum_examples = 0.\n",
        "\n",
        "                    print('begin validation ...')\n",
        "\n",
        "                    val_cum_loss = 0.\n",
        "                    val_cum_examples = 0\n",
        "\n",
        "                    count = 0\n",
        "                    NUM_BATCHES = 8  # number of batches to validate over each time\n",
        "                    for batch in batch_iter(val_data, batch_size, shuffle=True):\n",
        "                        if count >= NUM_BATCHES:\n",
        "                            break\n",
        "                        X, D, y = batch\n",
        "                        D = D.to(device)\n",
        "                        y = y.to(dtype=torch.float32, device=device)\n",
        "\n",
        "                        current_iter += 1\n",
        "                        train_iter += 1\n",
        "\n",
        "                        batch_size = len(X)\n",
        "                        out = model(X, D)\n",
        "                        val_loss = loss_fn(out, y)\n",
        "                        val_cum_loss += val_loss.item()\n",
        "                        val_cum_examples += batch_size\n",
        "                        count += 1\n",
        "\n",
        "                    val_losses.append(val_cum_loss / val_cum_examples)\n",
        "                    valid_metric = -val_cum_loss / val_cum_examples # metric for evaluating whether model is improving on val data\n",
        "\n",
        "                    print('validation: iter %d, val loss %f' % (train_iter, val_cum_loss / val_cum_examples))\n",
        "\n",
        "                    is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
        "                    hist_valid_scores.append(valid_metric)\n",
        "\n",
        "                    if is_better:\n",
        "                        patience = 0\n",
        "                        print('epoch %d, iter %d: save currently the best model to [%s]' %\n",
        "                                (epoch, train_iter, model_path))\n",
        "                        model.save(model_path)\n",
        "                        torch.save(optimizer.state_dict(), model_path + '.optim')\n",
        "                        np.save('{}_train.npy'.format(model_path.split('.')[-2]), np.array(train_losses))\n",
        "                        np.save('{}_val.npy'.format(model_path.split('.')[-2]), np.array(val_losses))\n",
        "                    elif patience < max_patience:\n",
        "                        patience += 1\n",
        "                        print('hit patience %d' % patience)\n",
        "\n",
        "                        if patience == max_patience:\n",
        "                            num_trial += 1\n",
        "                            print('hit #%d trial' % num_trial)\n",
        "                            if num_trial == max_num_trial:\n",
        "                                print('early stop!')\n",
        "                                exit(0)\n",
        "\n",
        "                            # decay lr, and restore from previously best checkpoint\n",
        "                            lr = optimizer.param_groups[0]['lr'] * lr_decay\n",
        "                            print('load previously best model and decay learning rate to %f' % lr)\n",
        "\n",
        "                            # load model\n",
        "                            params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "                            model.load_state_dict(params['state_dict'])\n",
        "                            model = model.to(device)\n",
        "                            train_losses = list(np.load('{}_train.npy'.format(model_path.split('.')[-2]), allow_pickle=True))\n",
        "                            val_losses = list(np.load('{}_val.npy'.format(model_path.split('.')[-2]), allow_pickle=True))\n",
        "\n",
        "                            print('restore parameters of the optimizers')\n",
        "                            optimizer.load_state_dict(torch.load(model_path + '.optim'))\n",
        "\n",
        "                            # set new lr\n",
        "                            for param_group in optimizer.param_groups:\n",
        "                                param_group['lr'] = lr\n",
        "\n",
        "                            # reset patience\n",
        "                            patience = 0\n",
        "\n",
        "        if epoch == max_epoch:\n",
        "            print('reached maximum number of epochs!')\n",
        "            break"
      ],
      "metadata": {
        "id": "kFhfncT-hruW"
      },
      "id": "kFhfncT-hruW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA USER INTERACTION AREA"
      ],
      "metadata": {
        "id": "48Iah0w1Xtzz"
      },
      "id": "48Iah0w1Xtzz"
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL SPECS\n",
        "WINDOW_SIZE = 3  # number of neighbors to consider in each direction\n",
        "EMB_DIM = 256  # dim each paragraph becomes, via magic :)\n",
        "EMB_METHOD = 'simcse'  # one of 'doc2vec', 'simcse', 'fasttext'\n",
        "MLP_ARCHITECTURE = [1024, 256, 64]  # sizes of hidden layers in MLP\n",
        "\n",
        "model = MLP(layer_dims=MLP_ARCHITECTURE, window_size=WINDOW_SIZE, emb_dim=EMB_DIM, emb_method=EMB_METHOD)"
      ],
      "metadata": {
        "id": "KOYIonqiYPSY"
      },
      "id": "KOYIonqiYPSY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING STUFF\n",
        "train_data, val_data, test_data = get_split_data(data, window_size=WINDOW_SIZE, train_ratio=0.65, val_ratio=0.3, test_ratio=0.01)\n",
        "train(model, train_data, val_data, lr=0.002, batch_size=128, grad_clip=5.0, lr_decay=0.5,\n",
        "      max_epoch=100, log_every=5, valid_niter=25, max_patience=5, max_num_trial=5, model_path='checkpoints/mlp_{}.bin'.format(EMB_METHOD))"
      ],
      "metadata": {
        "id": "B9KIlXR6RL7y"
      },
      "id": "B9KIlXR6RL7y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVAL STUFF\n",
        "params = torch.load('checkpoints/mlp_{}.bin'.format(EMB_METHOD), map_location=lambda storage, loc: storage)\n",
        "model.load_state_dict(params['state_dict'])\n",
        "model = model.to(device)\n",
        "\n",
        "# article = data[20]['paragraphs']\n",
        "sample = data[200:300]\n",
        "roots = [indexified_tree(model.outline(a['paragraphs'])) for a in sample]\n",
        "golds = [indexified_tree(a['tree'].root) for a in sample]\n",
        "lens = [len(a['paragraphs']) for a in sample]\n",
        "batch_lca_loss(roots, golds, lens)"
      ],
      "metadata": {
        "id": "3hy4A11xhwzd"
      },
      "id": "3hy4A11xhwzd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2PJZlXY0CR7F",
        "PDWF2apsjZT1"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}